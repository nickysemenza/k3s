apiVersion: helm.toolkit.fluxcd.io/v2beta1
kind: HelmRelease
metadata:
  name: prom
  namespace: monitoring
spec:
  interval: 5m
  chart:
    spec:
      # renovate: registryUrl=https://prometheus-community.github.io/helm-charts
      chart: kube-prometheus-stack
      version: 16.9.2
      sourceRef:
        kind: HelmRepository
        name: prometheus
        namespace: flux
      interval: 1m
  valuesFrom:
    - kind: Secret
      name: prom-operator-values
  values:
    alertmanager:
      ingress:
        enabled: true
        annotations:
          traefik.ingress.kubernetes.io/router.middlewares: system-cf-access-auth@kubernetescrd
        hosts:
          - alertmanager.k8s.nickysemenza.com
    grafana:
      sidecar:
        dashboards:
          enabled: true
          label: grafana_datasource
          searchNamespace: ALL
      ingress:
        enabled: true
        annotations:
          traefik.ingress.kubernetes.io/router.middlewares: system-cf-access-auth@kubernetescrd
        hosts:
          - grafana.k8s.nickysemenza.com
      grafana.ini:
        users:
          allow_sign_up: "false"
          auto_assign_org: "true"
          auto_assign_org_role: "Editor"
        auth:
          proxy:
            enabled: "true"
            header_name: "Cf-Access-Authenticated-User-Email"
            header_property: "email"
            auto_sign_up: "true"

      envValueFrom:
        GF_DATABASE_PASSWORD:
          secretKeyRef:
            name: grafana-db-credentials
            key: db-password
      env:
        # GF_TRACING_JAEGER_ADDRESS: "jaeger-tracing-agent.default.svc.cluster.local:6831"
        # GF_INSTALL_PLUGINS: "grafana-clock-panel,grafana-simple-json-datasource,jdbranham-diagram-panel,grafana-piechart-panel,natel-discrete-panel"
        GF_DATABASE_TYPE: postgres
        GF_DATABASE_NAME: "grafana-k8s"
        GF_DATABASE_HOST: "34.66.204.3"
        GF_DATABASE_USER: "postgres"
        GF_DATABASE_SSL_MODE: disable
      nodeSelector:
        location: home
      plugins:
        - natel-discrete-panel
      dashboardProviders:
        dashboardproviders.yaml:
          apiVersion: 1
          providers:
            - name: "default"
              orgId: 1
              folder: ""
              type: file
              disableDeletion: false
              editable: true
              options:
                path: /var/lib/grafana/dashboards/default
      dashboards:
        default:
          # https://github.com/k8s-at-home/k8s-cluster/blob/master/cluster/monitoring/prometheus-operator/prometheus-operator.yaml#L100
          flux:
            url: https://raw.githubusercontent.com/fluxcd/helm-operator/master/chart/helm-operator/dashboards/helm-operator-dashboard.json
            datasource: Prometheus
          vmware-cluster:
            url: https://raw.githubusercontent.com/pryorda/vmware_exporter/master/dashboards/cluster.json
            datasource: Prometheus
          vmware-esx:
            url: https://raw.githubusercontent.com/pryorda/vmware_exporter/master/dashboards/esx.json
            datasource: Prometheus
          vmware-esxi:
            url: https://raw.githubusercontent.com/pryorda/vmware_exporter/master/dashboards/esxi.json
            datasource: Prometheus
          vmware-virtualmachine:
            url: https://raw.githubusercontent.com/pryorda/vmware_exporter/master/dashboards/virtualmachine.json
            datasource: Prometheus
          # curl -s  'https://grafana.com/api/dashboards?orderBy=name&direction=asc&includeLogo=1&page=1&pageSize=100&dataSourceSlugIn=prometheus&filter=unifi-poller' | \
          # jq '.items[] as $item | {slug: $item.slug, revision: $item.revision, gnetId: $item.id}'
          unifi-poller-client-dpi:
            revision: 4
            gnetId: 11310
            datasource: Prometheus
          unifi-poller-client-insights:
            revision: 8
            gnetId: 11315
            datasource: Prometheus
          unifi-poller-network-sites:
            revision: 4
            gnetId: 11311
            datasource: Prometheus
          unifi-poller-uap-insights:
            revision: 9
            gnetId: 11314
            datasource: Prometheus
          unifi-poller-usg-insights:
            revision: 8
            gnetId: 11313
            datasource: Prometheus
          unifi-poller-usw-insights:
            revision: 8
            gnetId: 11312
            datasource: Prometheus
          go-processes:
            revision: 2
            gnetId: 6671
            datasource: Prometheus
          transmission:
            revision: 1
            gnetId: 8259
            datasource: Prometheus
          vmware-stats:
            revision: 1
            gnetId: 11243
            datasource: Prometheus
          node-exporter-server-metrics:
            revision: 8
            gnetId: 405
            datasource: Prometheus
          node-exporter-full:
            revision: 20
            gnetId: 1860
            datasource: Prometheus
          netdata:
            revision: 1
            gnetId: 7107
            datasource: Prometheus
    prometheusOperator:
      # prometheusConfigReloaderImage:
      # https://quay.io/repository/coreos/prometheus-config-reloader?tag=v0.20.0&tab=tags
      # tag: v0.39.0
      # nodeSelector:
      #   location: home
      # image:
      #   repository: raspbernetes/prometheus-operator
    prometheus-node-exporter:
      tolerations:
        - key: "arm"
          operator: "Exists"
    prometheus:
      ingress:
        enabled: true
        annotations:
          traefik.ingress.kubernetes.io/router.middlewares: system-cf-access-auth@kubernetescrd
        hosts:
          - prometheus.k8s.nickysemenza.com
      prometheusSpec:
        storageSpec:
          volumeClaimTemplate:
            spec:
              storageClassName: local-path
              accessModes: ["ReadWriteOnce"]
              resources:
                requests:
                  storage: 50Gi
        serviceMonitorSelectorNilUsesHelmValues: false
        podMonitorSelectorNilUsesHelmValues: false
        # additionalScrapeConfigs:
        #   - job_name: "netdata-scrape"
        #     metrics_path: "/api/v1/allmetrics"
        #     params:
        #       # format: prometheus | prometheus_all_hosts
        #       # You can use `prometheus_all_hosts` if you want Prometheus to set the `instance` to your hostname instead of IP
        #       format: [prometheus]
        #     honor_labels: true
        #     static_configs:
        #       - targets:
        #           - 10.0.0.11:19999
      additionalPodMonitors:
        - name: traefik-pod-monitor
          podMetricsEndpoints:
            - port: traefik
              path: /metrics
              interval: 15s
          selector:
            matchLabels:
              app: svclb-system-traefik
          namespaceSelector:
            matchNames:
              - system
        - name: traefik-pod-monitor2
          podMetricsEndpoints:
            - port: traefik
              path: /metrics
              interval: 15s
          selector:
            matchLabels:
              app.kubernetes.io/instance: system-traefik
          namespaceSelector:
            matchNames:
              - system
      additionalServiceMonitors:
        - name: kured
          namespaceSelector:
            matchNames:
              - system
          selector:
            matchExpressions:
              - key: app
                operator: In
                values:
                  - kured
          endpoints:
            - port: metrics
              interval: 15s
        - name: digitalocean-exporter
          namespaceSelector:
            matchNames:
              - exporters
          selector:
            matchExpressions:
              - {
                  key: app.kubernetes.io/name,
                  operator: In,
                  values: ["digitalocean-exporter"],
                }
          endpoints:
            - port: http
              interval: 60s
        - name: cloudflare-exporter
          namespaceSelector:
            matchNames:
              - exporters
          selector:
            matchLabels:
              app.kubernetes.io/name: cloudflare-exporter
          endpoints:
            - port: http
        - name: unifi-poller
          namespaceSelector:
            matchNames:
              - exporters
          selector:
            matchLabels:
              app.kubernetes.io/name: unifi-poller
          endpoints:
            - port: http
    kubeControllerManager:
      enabled: false
    kubeEtcd:
      enabled: false
    kubeProxy:
      enabled: false
    kubeScheduler:
      enabled: false
---
apiVersion: helm.toolkit.fluxcd.io/v2beta1
kind: HelmRelease
metadata:
  name: snmp
  namespace: monitoring
spec:
  interval: 5m
  chart:
    spec:
      # renovate: registryUrl=https://prometheus-community.github.io/helm-charts
      chart: prometheus-snmp-exporter
      version: 0.1.2
      sourceRef:
        kind: HelmRepository
        name: prometheus
        namespace: flux
      interval: 1m
  values:
    image:
      tag: master
    ingress:
      enabled: true
      annotations:
        traefik.ingress.kubernetes.io/router.middlewares: system-cf-access-auth@kubernetescrd
      hosts:
        - snmp.k8s.nickysemenza.com
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: "snmp-synology"
  namespace: monitoring
spec:
  endpoints:
    - honorLabels: true
      params:
        module:
          - synology
        target:
          - cantaloupe.lan.nickysemenza.com
      path: /snmp
      port: http
      scrapeTimeout: 10s
      relabelings:
        - sourceLabels: [__param_target]
          targetLabel: instance
    - honorLabels: true
      params:
        module:
          - synology
        target:
          - synology-files.ts.nickysemenza.com
      path: /snmp
      port: http
      scrapeTimeout: 30s
      relabelings:
        - sourceLabels: [__param_target]
          targetLabel: instance
  selector:
    matchLabels:
      app.kubernetes.io/name: prometheus-snmp-exporter
---
# https://snmp.k8s.nickysemenza.com/snmp?target=10.0.0.1&module=mikrotik
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: "snmp-mikrotik"
  namespace: monitoring
spec:
  endpoints:
    - honorLabels: true
      params:
        module:
          - mikrotik
        target:
          - 10.0.0.1
      path: /snmp
      port: http
      interval: 10s
      scrapeTimeout: 10s
      relabelings:
        - sourceLabels: [__param_target]
          targetLabel: instance
  selector:
    matchLabels:
      app.kubernetes.io/name: prometheus-snmp-exporter
---
# https://snmp.k8s.nickysemenza.com/snmp?target=10.0.0.33&module=ubiquiti_unifi#
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: "snmp-unifi"
  namespace: monitoring
spec:
  endpoints:
    - honorLabels: true
      params:
        module:
          - ubiquiti_unifi
        target:
          - 10.0.0.33
      path: /snmp
      port: http
      interval: 10s
      scrapeTimeout: 10s
      relabelings:
        - sourceLabels: [__param_target]
          targetLabel: instance
  selector:
    matchLabels:
      app.kubernetes.io/name: prometheus-snmp-exporter
# reqs:
# https://github.com/SumoLogic/sumologic-kubernetes-collection/pull/1310/files
# https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack#upgrading-an-existing-release-to-a-new-major-version
# kubectl apply -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/release-0.43/example/prometheus-operator-crd/monitoring.coreos.com_alertmanagers.yaml
# kubectl apply -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/release-0.43/example/prometheus-operator-crd/monitoring.coreos.com_podmonitors.yaml
# kubectl apply -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/release-0.43/example/prometheus-operator-crd/monitoring.coreos.com_probes.yaml
# kubectl apply -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/release-0.43/example/prometheus-operator-crd/monitoring.coreos.com_prometheuses.yaml
# kubectl apply -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/release-0.43/example/prometheus-operator-crd/monitoring.coreos.com_prometheusrules.yaml
# kubectl apply -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/release-0.43/example/prometheus-operator-crd/monitoring.coreos.com_servicemonitors.yaml
# kubectl apply -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/release-0.43/example/prometheus-operator-crd/monitoring.coreos.com_thanosrulers.yaml
